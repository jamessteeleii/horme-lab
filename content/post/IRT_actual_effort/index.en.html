---
title: "Actual Effort in Cognitive Tasks"
subtitle: Can Item Response Theory help with operationalisation?
date: '2022-01-29T16:06:15Z'
categories: []
tags: []
summary: Item Response Theory assumes latent abilities, and varying difficulties for
  items... both could be useful, given my conceptual definition, for operationalising
  actual effort
authors: []
lastmod: '2022-03-16T09:26:03Z'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<p>In my article titled <a href="https://www.hormelab.com/what_is_effort/">“What is (perception of) effort? Objective and subjective effort during attempted task performance”</a> I offer clear conceptual definitions of both <em>actual</em> effort (objective) and <em>perception of</em> effort (subjective). Clear conceptual definitions are key for determining whether a given operationalisation of those definitions (i.e., our ways of defining those variables in the context of our research) meet the necessary and sufficient conditions adequately for the theoretical unit of interest.</p>
<p>In this post I will attempt to provide a solution for a problem I have thought about for a while; namely, how best to operationalise actual effort in cognitive tasks. As I will explain, unlike many physical tasks where operationalisation is fairly trivial, it is not quite so simple to do so for tasks where the underlying capacity that disposes an individual to be able to attempt and perhaps complete the task is not directly observable nor is the demand that the task presents.</p>
<p>However, I think that a solution might lie in the measurement theory employed in psychometrics known as Item Response Theory (IRT). Some of it’s key assumptions and the parameters of its models map conceptually well to those underlying my definition of actual effort. First though, let’s review my definition for context</p>
<div id="conceptual-definition-of-actual-effort" class="section level3">
<h3><em>Conceptual definition of actual effort</em></h3>
<p>I define actual effort as follows:</p>
<blockquote>
<p>“Effort; <em>noun</em>;
That which must be done in attempting to meet a particular task demand, or set of task demands, and which is determined by the current task demands relative to capacity to meet those demands, though cannot exceed that current capacity.”</p>
</blockquote>
<p>And more specifically following Markus’<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> Set Theoretical approach as<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>:</p>
<blockquote>
<p>“Effort (concept); <span class="math inline">\(E_{A}(j, t, C_{A}, D_{A}, w, T_{Any})\)</span> is the actual effort for any individual <span class="math inline">\(j\)</span> at time <span class="math inline">\(t\)</span> where <span class="math inline">\(C_{A}(j, t, x_{C})\)</span>, and <span class="math inline">\(D_{A}(i, t, x_{D})\)</span> are the actual capacity and actual demands respectively, and <span class="math inline">\(x_{C}\)</span> and <span class="math inline">\(x_{D}\)</span> are the magnitudes of those respectively for individual <span class="math inline">\(j\)</span> at time <span class="math inline">\(t\)</span>, where <span class="math inline">\(w\)</span> denotes all possible states of affairs (i.e. combinations of <span class="math inline">\(j, t, C_{A}\)</span>, and <span class="math inline">\(D_{A})\)</span>, and <span class="math inline">\(T_{Any}\)</span> denotes the boundary conditions noting it as intensional to all possible types of tasks.”</p>
</blockquote>
<p>And which is expressed as a derived ratio, given that capacity and demands have natural origins (capacity can be zero, as can demands, but neither can be less than zero):</p>
<p><span class="math display">\[D_{A} \leq C_{A} \Rightarrow E_{A} = \frac{ 
    D_{A}}
{C_{A}} \times 100%\]</span></p>
<p><span class="math display">\[D_{A} &gt; C_{A} \Rightarrow E_{A} = 100%\]</span></p>
<p>Where the ratio is expressed as a percentage (%).</p>
<p>Now, the <span class="math inline">\(T_{Any}\)</span> argument is important for what I am about to present. The conceptual definitions I have developed are deliberately agnostic of the type of task being performed. However, it’s not that easy to apply the definition to all kinds of tasks given that in many we haven’t got good operationalisations of <span class="math inline">\(C_{A}\)</span> and <span class="math inline">\(D_{A}\)</span>. Sure, for tasks typical to my alma mater of research, resistance training, it’s pretty simple. Take a physical task such as this, in essence lifting a weight… here’s how I gave an example of the definition in play:</p>
<blockquote>
<p>“In a physical task the role of differential demands and capacity are easily considered in that actual effort is determined by the task demands relative to the current capacity to meet task demands. As such, if two individuals were attempting to pick up the same specific absolute load (e.g. 80 kg) the stronger of the two would initially require less actual effort to complete this task. If they had both performed prior tasks that had resulted in a reduction in their maximal strength, then each would require a greater actual effort to complete the task than compared with when their capacity was not reduced. And further, if both continued performing repetitions of this task their maximal strength might continue to reduce insidious to continued attempts to maintain a particular absolute demand, and thus require an increasingly greater actual effort with every individual or continued attempt to meet the task demands. Correspondingly, if the absolute task demands were increased then both individuals would also require greater actual effort to complete the task. Yet for both, the continued attempted performance of the task with fixed absolute demands and insidious reduction of capacity or the increase of absolute demands, task performance would be capped by their maximum capacity at which maximum effort is required. With training though that maximum strength might be increased such that a given absolute task demand now represents relatively less and so requires less actual effort. Further, biomechanical alterations to the task might reduce the absolute demands and thus the actual effort.”</p>
</blockquote>
<p>So, let’s say an individual <span class="math inline">\((j)\)</span> did try to lift a load <span class="math inline">\((i)\)</span> that was 80 kg. And let’s say that the maximum load they could lift was 100 kg. Well, it’s pretty simple to calculate the actual effort required:</p>
<p><span class="math display">\[D_{A(i)} = 80, C_{A(j)} = 100\]</span>
<span class="math display">\[E_{A(ji)} = \frac{ 
D_{A(i)}}
{C_{A(j)}} \times 100%\]</span>
<span class="math display">\[\ 80 = \frac{ 
80}
{100} \times 100\]</span></p>
<p>So, the amount of actual effort required by the individual to lift the load is 80%. Nice and simple.</p>
<p>But what about cognitive tasks? Sure, we can conceivably apply my definition to such tasks if we assume that such tasks present demands that must be met, and that we have some capacity to meet them. In fact, we could draw similar examples as above for such tasks… again, here’s what I note:</p>
<blockquote>
<p>“Similar examples could be provided for cognitive tasks. For example, if two individuals were attempting to hold a fixed number of items in their working memory, the one who has the larger working memory of the two would require less actual effort to complete this task. However, both individuals would again require greater actual effort to do so in the presence of lingering reduction in cognitive capacity from prior tasks, or from continued attempts to meet the task demands, or from increased absolute task demands (i.e., more items to be held in working memory). Again, training may also improve maximal capacity. Also, cognitive processing alterations (i.e., heuristics; Shah and Oppenheimer, 2008) might reduce task demands and thus the actual effort.”</p>
</blockquote>
<p>The problem however, is actually measuring the capacity being used to perform cognitive tasks, and the demands of those tasks. It’s not as simple as with say strength or the load lifted. We have an operationalisation problem for cognitive tasks.</p>
<p>But, as noted, I think the trick to this problem might lie in IRT.</p>
</div>
<div id="item-response-theory" class="section level3">
<h3><em>Item Response Theory</em></h3>
<p>For those unfamiliar with IRT, I’ll provide a very brief overview of some key elements that are relevant for this post. But otherwise there are plenty of great texts out there covering its background and history, differences with Classical Test Theory, assumptions, different model types and parameters, how these are estimated, model fit etc<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>.</p>
<p>Let’s suppose that, for each person we test <span class="math inline">\(j(j=1,...,J)\)</span>, and each item in a test they complete <span class="math inline">\(i(i=1,...,I)\)</span>, we have a binary response <span class="math inline">\(y_{ji}\)</span> which is coded 1 for a correct answer (i.e., success), and 0 for an incorrect answer (i.e., failure). A binary IRT model aims to model <span class="math inline">\(p_{ji} = P(y_{ji}=1)\)</span>; in essence the probability that a person <span class="math inline">\(j\)</span> correctly answers item <span class="math inline">\(i\)</span> which is assumed to follow a Bernoulli distribution:</p>
<p><span class="math display">\[\ y_{ji}\sim Bernoulli(p_{ji})\]</span></p>
<p>A key aspect of IRT is that, as a theory, it posits links between some construct that is a characteristic of an individual, referred to as a <em>trait</em> or <em>ability</em>, and that an individuals performance in a test of that ability are predicted or explained by that ability. However, we can’t directly observe this ability itself and instead must infer an estimation of it from the observation of performance on the test. For this reason the abilities are often referred to as <em>latent</em>. The relationship between the “observable” and “unobservable” is then described by a mathematical function which are models that make specific assumptions about the test data; different models imply different assumptions one is willing to make about the test data being examined given the nature of the test conducted. For example, a recently popular model due to its flexibility is the four-parameter logistic model (4PL) where <span class="math inline">\(P(y_{ji}=1)\)</span> is expressed through the equation:</p>
<p><span class="math display">\[\ P(y_{ji}=1)=\gamma+(1-\gamma_{i}-\psi_{i}) \frac{
1}{1+exp(-(\alpha_{i}\theta_{j}-\beta_{i}))}\]</span></p>
<p>In this model there are four key parameters as the name suggests, which reflect the assumptions about the test data. The <span class="math inline">\(\beta_{i}\)</span> parameter describes the item location which depending on the sign direction people prefer can refer to either the ‘difficulty’ or the ‘easiness’ of the item<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>. The <span class="math inline">\(\alpha_{i}\)</span> parameter refers to how strongly an item is related to the latent ability <span class="math inline">\(\theta_{j}\)</span> which is typically positive (i.e., that answering correctly typically implies higher ability than if answering incorrectly).The parameters <span class="math inline">\(\gamma_{i}\)</span> and <span class="math inline">\(\psi_{i}\)</span> refer to the guessing probability (i.e., that the correct answer on an item could be guessed and not due to ability), and a lapse probability respectively (i.e., that a person could make a mistake, press the wrong key etc. despite actually knowing the correct answer).</p>
<p>An Item Characteristic Curve (ICC) is usually used to visualise the relationship between ability and the probability of a correct response to items. So for example, a 4PL model might look something like:</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Different models make different assumptions about these parameters. For example, the simplest one-parameter logistic model (1PL or Rasch model) assumes that <span class="math inline">\(\alpha=1\)</span> and both <span class="math inline">\(\gamma=0\)</span> and <span class="math inline">\(\psi=0\)</span>; that is to say, items discriminate between higher and lower abilities equally well and there is no guessing or lapses occurring. A model of that kind might look like:</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>In general though, the application of an IRT model allows for test data to be decomposed into an estimate of the characteristic of the individual (i.e., their <em>ability</em>, <span class="math inline">\(\theta_{j}\)</span>). For my purpose today though, the other parameter of such models that is of primary iterest is the estimates of the test items location (i.e., their <em>difficulty</em>, <span class="math inline">\(\beta_{i}\)</span>).</p>
<p>That’s about as detailed as I am going to get for the purpose of this post. The point I want to make is more conceptual… or at least, it is more about exploring whether or not we can use IRT models as a means of operationalising <span class="math inline">\(C_{A(j)}\)</span> and <span class="math inline">\(D_{A(i)}\)</span> in order to operationalise calculation of <span class="math inline">\(E_{A(ji)}\)</span> under cognitive tasks.</p>
</div>
<div id="ability-capacity-theta_jc_aj-difficulty-demands-beta_id_ai" class="section level3">
<h3><em>Ability = Capacity (<span class="math inline">\(\theta_{j}=C_{A(j)}\)</span>); Difficulty = Demands (<span class="math inline">\(\beta_{i}=D_{A(i)}\)</span>)</em></h3>
<p>In essence, the argument I am putting forward here is that the two primitives <span class="math inline">\(C_{A}\)</span> and <span class="math inline">\(D_{A}\)</span> I have assumed are necessary and sufficient for the derivation of the concept <span class="math inline">\(E_{A}\)</span> are exactly what IRT takes as its own underlying assumed constructs; namely <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\beta\)</span> respectively.</p>
<p>So I think that we can use IRT models in order to estimate these parameters and then use them to calculate an estimate of the <span class="math inline">\(E_{A}\)</span> required for each individual as they attempt each item. That is to say, we can take our function for <span class="math inline">\(E_{A}\)</span> above and say:</p>
<p><span class="math display">\[\beta \leq \theta \Rightarrow E_{A(IRT)} = \frac{ 
    \beta}
{\theta} \times 100%\]</span></p>
<p><span class="math display">\[\beta &gt; \theta \Rightarrow E_{A(IRT)} = 100%\]</span>
Where the IRT subscript denotes that it is estimated from the IRT model.</p>
<p>However, those who are familiar with typical IRT models might notice a problem here for my proposed solution to operationalisation; <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\theta\)</span> are typically estimated on the <span class="math inline">\(logit\)</span> scale which ranges <span class="math inline">\(-\infty,\infty\)</span>. This poses some problems for us when it comes to calculating <span class="math inline">\(E_{A}\)</span> given it is a ratio and the <span class="math inline">\(logit\)</span> scale does not have ratio properties.</p>
<p>Fortunately, the choice of which scale to place <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\theta\)</span> on is rather arbitrary. Most common IRT models can undergo transformations from the <span class="math inline">\(logit\)</span> scale that both preserves the underlying probabilities for a given <span class="math inline">\(\theta\)</span> completing a given item with difficulty <span class="math inline">\(\beta\)</span>. For example, the 1PL model can undergo linear transformation without altering the underlying mathematical model and we can have <span class="math inline">\(\theta_{transf}\)</span> and <span class="math inline">\(\beta_{transf}\)</span>. So this gave me an idea, a toy example of sorts, to explore whether or not the derivation of effort from an IRT type model would provide a good estimate of the actual effort resultant from the demands of the task items, and the latent ability of the person performing them.</p>
</div>
<div id="using-irt-models-for-lifting-weights" class="section level3">
<h3><em>Using IRT models for lifting weights</em></h3>
<p>I’m a fan of analogical abduction. I used it in developing my conceptual definition of effort in the first place. So I’m going to use the analogy of a test of the ability ‘strength’ where an individual attempts to lift different loads. In resistance training, it is pretty common to measure strength this way. We perform what is referred to as a <em>one-repetition maximum</em> (1RM) test. This test is the operationalisation of strength through the capacity to lift load in a given exercise task. Normally, an individual would perform a warmup, and then lift progressively heavier and heavier loads<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> until the heaviest load they could lift only once, and no more, was identified. If we know the maximum load that an individual can lift once, then it’s a good assumption to think that this means they can also lift any load weighing less than this. If their 1RM was 100 kg and we asked them to lift 50 kg they’d almost certainly be able to. If we asked them to lift 90 kg, whilst it would be a lot more demanding to do so, they’d still almost certainly be able to do so. But if we asked them to lift 110 kg they’d almost certainly not be able to do so.</p>
<p>Given that the outcome of attempting to lift a given load can be considered in a binary manner (that is to say a person either can, or cannot, lift the load given their strength ability), whilst unusual to do so, we could fit an IRT model to such data. This offers an interesting toy example to play with where we could actually have a directly <em>measurable ability</em> (<em>strength</em>; <span class="math inline">\(C_{A(j)}\)</span>), and also can directly <em>measure item difficulty</em> (<em>loads</em>; <span class="math inline">\(D_{A(i)}\)</span>). So we can directly calculate <span class="math inline">\(E_{A(j)}\)</span> from such data. If we also fit an IRT model, transform the estimates of <span class="math inline">\(\theta_{j}\)</span> and <span class="math inline">\(\beta_{i}\)</span> back to the raw scale (i.e., kg; <span class="math inline">\(\theta_{transf}\)</span> and <span class="math inline">\(\beta_{transf}\)</span>), we can then also calculate an estimate of actual effort based on the model (<span class="math inline">\(E_{A(j,IRT)}\)</span>).</p>
<p>Let’s simulate some data<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> and see what it looks like. We sample<span class="math inline">\(\ n=100\)</span> individuals from a population with a 1RM for the exercise task of <span class="math inline">\(\mu=100\)</span> kg and <span class="math inline">\(sd=25\)</span> kg, which is pretty reasonable for such a measure. We’ll say that each individual completes twenty ‘items’, which in this case means they attempt to lift twenty different loads, ranging from 10 kg to 200 kg in 10 kg intervals. Given we know their strength (i.e., <span class="math inline">\(C_{A(j)}\)</span>) and the load they are trying to lift (i.e., <span class="math inline">\(D_{A(j)}\)</span>) we can calculate their actual effort (i.e., <span class="math inline">\(E_{A(ji)}\)</span>). Further, we can also code for their binary ‘response’ to each load; that is to say, whether they successfully lift it or not. This is determined by their strength, the same way that response to items in IRT models is assumed to be determined by the latent ability. In this case though, we know for certain whether they can or can’t lift the load given we actually know their 1RM. So we simply code 1 if 1RM is greater than or equal to the load, and 0 if 1RM is less than the load.</p>
<p>An example of data for an individual showing the first 10 loads looks like this<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>:</p>
<pre><code>## # A tibble: 10 x 5
##    person one_RM item  actual_effort response
##    &lt;fct&gt;   &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;    &lt;dbl&gt;
##  1 p002     91.5 10             10.9        1
##  2 p002     91.5 20             21.8        1
##  3 p002     91.5 30             32.8        1
##  4 p002     91.5 40             43.7        1
##  5 p002     91.5 50             54.6        1
##  6 p002     91.5 60             65.5        1
##  7 p002     91.5 70             76.5        1
##  8 p002     91.5 80             87.4        1
##  9 p002     91.5 90             98.3        1
## 10 p002     91.5 100           100          0</code></pre>
<p>Now, given the kind of test completed here (i.e., lifting different loads) it is reasonable to use the 1PL model mentioned above because there is no guessing or lapsing, nor do different items differentiate people of different abilities differently (it’s a toy example so we’re building these assumptions in). I’m going to follow an approach using Bayesian mixed effect modelling using weakly regularising priors to fit the initial 1PL model using the {brms} package which I won’t go into detail about here<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>. I’ll use Bayesian models for the following parts also with default priors to keep the approach consistent.</p>
<p>So we fit the 1PL model and we can plot the ICCs<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> for each load which look like this:</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>So it’s pretty clear that the model recognises that people are more likely to lift lighter loads than heavier loads, and that people with a greater strength ability are also more likely to lift a given load. We do have some loads though that are just incredibly easy such that pretty much anyone can lift them, and conversely so incredible difficult that no one can lift them.</p>
<p>After we fit the 1PL model we we can then extract the random effects by person and item, namely <span class="math inline">\(\theta_{j}\)</span> and <span class="math inline">\(\beta_{i}\)</span>. What we want to do is examine their relationship with the ‘measured’ raw 1RM and loads respectively so we can determine the linear transformation needed to convert <span class="math inline">\(\theta_{j}\)</span> and <span class="math inline">\(\beta_{i}\)</span> back to the raw kg units <span class="math inline">\(\theta_{j(transf)}\)</span> and <span class="math inline">\(\beta_{i(transf)}\)</span>.</p>
<p>So we fit a simple linear model to estimate 1RM from <span class="math inline">\(\theta_{j}\)</span>:</p>
<pre class="r"><code>lm_1RM</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: one_RM ~ theta 
##    Data: scores_oneRM (Number of observations: 100) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   101.60      0.40   100.82   102.36 1.00     3512     2914
## theta         3.72      0.06     3.61     3.84 1.00     4520     3043
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     4.07      0.30     3.54     4.69 1.00     3747     2774
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>And also to estimate load from <span class="math inline">\(\beta_{i}\)</span>:</p>
<pre class="r"><code>lm_loads</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: load ~ beta 
##    Data: loads (Number of observations: 20) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   103.07      1.84    99.54   106.85 1.00     2929     2190
## beta          3.66      0.11     3.43     3.88 1.00     3147     2548
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     7.92      1.47     5.64    11.38 1.00     2759     2318
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>And visually the fit looks like this:</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>So, we can now use the intercepts and coefficients from these models to linearly transform <span class="math inline">\(\theta_{j}\)</span> and <span class="math inline">\(\beta_{i}\)</span> back to the raw kg units <span class="math inline">\(\theta_{j(transf)}\)</span> and <span class="math inline">\(\beta_{i(transf)}\)</span>. Then we can use them to calculate our IRT model estimate of actual effort as:</p>
<p><span class="math display">\[\beta_{i(transf)} \leq \theta_{j(transf)} \Rightarrow E_{A(ji,IRT)} = \frac{ 
    \beta_{i(transf)}}
{\theta_{j(transf)}} \times 100%\]</span></p>
<p><span class="math display">\[\beta_{i(transf)} &gt; \theta_{j(transf)} \Rightarrow E_{A,(ji,IRT)} = 100%\]</span></p>
<p>Now we have two sets of actual effort in our dataset; we have the original actual effort calculated directly from the 1RM and loads (<span class="math inline">\(E_{A(ji)}\)</span>) and also and effort calculated from our transformed estimates of ability and difficulty (<span class="math inline">\(E_{A(ji,IRT)}\)</span>).</p>
<pre><code>##    person   one_RM item actual_effort   theta        beta theta_to_raw
## 1    p001 103.3932   10      9.671814 1.25077 -23.9547152     106.2544
## 2    p001 103.3932   20     19.343628 1.25077 -23.9230662     106.2544
## 3    p001 103.3932   30     29.015442 1.25077 -16.8780428     106.2544
## 4    p001 103.3932   40     38.687256 1.25077 -16.8764711     106.2544
## 5    p001 103.3932   50     48.359070 1.25077 -16.8987563     106.2544
## 6    p001 103.3932   60     58.030884 1.25077 -13.2056382     106.2544
## 7    p001 103.3932   70     67.702698 1.25077 -10.1221819     106.2544
## 8    p001 103.3932   80     77.374512 1.25077  -6.3022157     106.2544
## 9    p001 103.3932   90     87.046326 1.25077  -3.8182128     106.2544
## 10   p001 103.3932  100     96.718140 1.25077  -0.2936207     106.2544
##    beta_to_raw irt_effort
## 1     15.47079   14.56013
## 2     15.58653   14.66906
## 3     41.34980   38.91583
## 4     41.35555   38.92124
## 5     41.27405   38.84454
## 6     54.77959   51.55511
## 7     66.05562   62.16740
## 8     80.02504   75.31454
## 9     89.10890   83.86370
## 10   101.99815   95.99425</code></pre>
<p>Now, because we have multiple observations for each individuals we can fit a mixed effects model with random intercepts and slopes to explore how well the <span class="math inline">\(E_{A(ji,IRT)}\)</span> estimates <span class="math inline">\(E_{A(ji)}\)</span>. Given the conceptualisation of effort as being a bounded construct on the 0% to 100% interval we should use a model taking that into account and so we fit an ordered beta regression model<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>.</p>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>This doesn’t look too bad to be fair. In fact, while it slightly underestimates at the lower bounds, it’s pretty darn good in my opinion and leaves me feeling fairly confident in using <span class="math inline">\(E_{A(ji,IRT)}\)</span> as my operationalisation for <span class="math inline">\(E_{A(ji)}\)</span>.</p>
</div>
<div id="summary-and-conclusion" class="section level3">
<h3><em>Summary and Conclusion</em></h3>
<p>Key assumptions underlying IRT models and their parameters, <em>ability</em> and <em>difficulty</em>, map conceptually well onto the assumed primitives, <em>capacity</em> and <em>demands</em>, in my definition of effort. Given this, IRT models seem like a useful approach to operationalisation of <em>actual effort</em> in cognitive tasks. In fact, using the example of a test involving lifting weights where we actually know a persons underlying ability/capacity and the difficulty/demands of each item in the test, the estimates of effort that result from an IRT model are pretty reasonable estimates of the actual effort we could calculate from direct measurements.</p>
<p>I think this approach offers an interesting opportunity to look at actual effort in cognitive tasks. This could be particularly useful in exploring psychophysics in such tasks where we also capture self-reports of perception of effort during each item.</p>
<p>Further, I do not think that such models are limited to only cognitive tasks. As I have shown here, we can apply IRT models to tasks we wouldn’t typically think to. Many people have been thwarted in attempts to conceptualise effort in target based tasks such as dart throwing. However, I think that the use of IRT models could also allow the estimation of actual effort for these tasks.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Markus, K. A. (2008). Constructs, concepts and the worlds of possibility: Connecting the measurement, manipulation, and meaning of variables. Measurement: Interdisciplinary Research and Perspectives, 6(1-2), 54–77. <a href="https://doi.org/10.1080/15366360802035513" class="uri">https://doi.org/10.1080/15366360802035513</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>I’ll use<span class="math inline">\(j\)</span> here to denote the individual instead of<span class="math inline">\(i\)</span> as I do in the original paper, to be in keeping with the typical notation used in Item Response Theory models that follows because <span class="math inline">\(i\)</span> is used for the ‘item’.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>To be fair, the <a href="https://en.wikipedia.org/wiki/Item_response_theory">Wikipedia page</a> on IRT provides a pretty good overview as expected. But I do like Fundamentals of Item Response Theory by R. K. Hambleton, H. Swaminathan, and H. J. Rogers as a strong intro text that’s pretty short (only ~150 pages excluding appendices etc.)<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Note, for my purposes here I am going with the ‘difficulty’, but if we wanted ‘easiness’ we would use <span class="math inline">\(\alpha_{i}\theta_{j}+\beta_{i}\)</span>.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>For those unfamiliar, in practice this is normally achieved within 3-5 attempts so as to not allow cumulative fatigue to unduly influence the estimate of maximum strength.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>I used Lisa DeBruine’s great package {faux} for this, which I tend to use for a lot of simulation as it’s so intuitive. Check it out <a href="https://debruine.github.io/faux/">here</a>.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>I’ve deliberately chose someone with a 1RM &lt; 100 kg so it’s clear that the response is dependent on the relationship between that and the load lifted.<a href="#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>But, take a look at at the great papers by by Paul Bürkner <a href="https://arxiv.org/abs/1905.09501">here</a> and <a href="https://www.mdpi.com/2079-3200/8/1/5/htm">here</a> who authored the {brms} package - see <a href="https://www.jstatsoft.org/article/view/v080i01">here</a>.<a href="#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>Credit to Solomon Kurz for his great <a href="https://solomonkurz.netlify.app/post/2021-06-29-make-icc-plots-for-your-brms-irt-models/">post</a> on wrangling {brms} models to create these ICC plots.<a href="#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>Nice paper on this recently developed approach to handling bounded variables, along with the {ordbetareg} package that overlays {brms}, from Robert Kubinec <a href="https://osf.io/preprints/socarxiv/2sx6y/">here</a><a href="#fnref10" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
